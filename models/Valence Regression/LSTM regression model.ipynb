{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3020c80",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6a9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers, models, losses, optimizers,callbacks\n",
    "\n",
    "from keras.layers import Dense,LSTM\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f890099",
   "metadata": {},
   "source": [
    "# load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e137d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158353, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22651b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_lyrics'] = df['clean_lyrics'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0188c",
   "metadata": {},
   "source": [
    "# Create X and y feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6859345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "X = df['clean_lyrics']\n",
    "\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2316e3a",
   "metadata": {},
   "source": [
    "# Vectorize X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5975eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158353, 300)\n"
     ]
    }
   ],
   "source": [
    "# Limiting our tokenizers vocab size\n",
    "max_words = 10000\n",
    " \n",
    "    \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "\n",
    "# Fit the tokenizer\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "\n",
    "# Create the sequences for each sentence, basically turning each word into its index position\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "\n",
    "index_word = tokenizer.index_word\n",
    "\n",
    "\n",
    "# # Limiting our sequencer to only include 300 words\n",
    "max_length = 300\n",
    "\n",
    "\n",
    "# # Convert the sequences to all be the same length of 300\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ce2b7",
   "metadata": {},
   "source": [
    "# Split train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "772d6c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((126682, 300), (126682,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc713a46",
   "metadata": {},
   "source": [
    "# Building LSTM nueral net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78548331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 32)           320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 300, 50)           16600     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356,851\n",
      "Trainable params: 356,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\stephen williams\\downloads\\nlp-extra-credit-main\\env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# This creates the Neural Network\n",
    "model = Sequential() \n",
    "\n",
    "# This embedding layer basically will automatically create the word2vec vectors based on your text data.\n",
    "model.add( Embedding(max_words, 32, input_length=max_length) ) \n",
    "\n",
    "model.add(LSTM(50,return_sequences=True,dropout =0.2))\n",
    "model.add(LSTM(50,dropout =0.2))\n",
    "model.add(Dense(1,kernel_initializer='normal',  activation='linear'))\n",
    "optimizer = optimizers.Adam(lr=0.003)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse']) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e9581",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db4cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\stephen williams\\downloads\\nlp-extra-credit-main\\env\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5068/5068 [==============================] - 548s 107ms/step - loss: 0.0618 - mse: 0.0618 - val_loss: 0.0554 - val_mse: 0.0554\n",
      "Epoch 2/15\n",
      "5068/5068 [==============================] - 577s 114ms/step - loss: 0.0535 - mse: 0.0535 - val_loss: 0.0518 - val_mse: 0.0518\n",
      "Epoch 3/15\n",
      "5068/5068 [==============================] - 587s 116ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.0507 - val_mse: 0.0507\n",
      "Epoch 4/15\n",
      "5068/5068 [==============================] - 582s 115ms/step - loss: 0.0486 - mse: 0.0486 - val_loss: 0.0506 - val_mse: 0.0506\n",
      "Epoch 5/15\n",
      "5068/5068 [==============================] - 562s 111ms/step - loss: 0.0468 - mse: 0.0468 - val_loss: 0.0506 - val_mse: 0.0506\n",
      "Epoch 6/15\n",
      "5068/5068 [==============================] - 524s 103ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.0501 - val_mse: 0.0501\n",
      "Epoch 7/15\n",
      "5068/5068 [==============================] - 541s 107ms/step - loss: 0.0437 - mse: 0.0437 - val_loss: 0.0509 - val_mse: 0.0509\n",
      "Epoch 8/15\n",
      "5068/5068 [==============================] - 560s 110ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0528 - val_mse: 0.0528\n"
     ]
    }
   ],
   "source": [
    "callback = callbacks.EarlyStopping(monitor='val_mse',patience = 2,restore_best_weights=True)\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 validation_split=0.2,\n",
    "                 epochs=15, batch_size=20,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_Valence_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4bb704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('LSTM_Valence_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b3a59",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bf0bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mse with stacked LSTM: 0.05180038884282112\n"
     ]
    }
   ],
   "source": [
    "mse= model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "\n",
    "print('Test mse with stacked LSTM:', mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
