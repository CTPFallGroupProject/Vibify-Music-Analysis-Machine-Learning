{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valence Prediction Conclusions:\n",
    "\n",
    "### Tesing Gradient Boosting Regressors\n",
    "\n",
    "Not much gained or lost from increase in sample size. \n",
    "R2 error increased by .018 which is not too significant \n",
    "\n",
    "on sample size 500/genre and NO hyperparameter tuning (out of the box):\n",
    "- [GB] Mean Squared Error: 0.05264719893512396\n",
    "- [GB] R2: 0.08043425668821236\n",
    "\n",
    "on sample size 1687/genre with optimal hyperparamers: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}:\n",
    "- [GB] Mean Squared Error: 0.05200244352093515\n",
    "- [GB] R2: 0.09854691815352778\n",
    "\n",
    "on full data set with optimal hyperparamers:\n",
    "- [GB] Mean Squared Error: 0.05284822264086245\n",
    "- [GB] R2: 0.12264012250017176\n",
    "\n",
    "#### ** GB runs MUCH faster than RFreg and produces better r2 on the full data set\n",
    "\n",
    "---\n",
    "\n",
    "### Tesing Random Forest Regressors\n",
    "\n",
    "Not much gained or lost from increase in sample size. \n",
    "R2 error increased by .02 which is not too significant \n",
    "\n",
    "on sample size 500/genre with optimal hyperparamers:\n",
    "- [RF] Mean Squared Error: 0.05222244210798196\n",
    "- [RF] R2: 0.10795588207769391\n",
    "\n",
    "on sample size 1687/genre with optimal hyperparamers:\n",
    "- [RF] Mean Squared Error: 0.05101035414841959\n",
    "- [RF] R2: 0.1205852034338123\n",
    "\n",
    "on full data set with optimal hyperparamers:\n",
    "- [RF] Mean Squared Error: 0.05115798019890316\n",
    "- [RF] R2: 0.15070068589697727\n",
    "\n",
    "---\n",
    "\n",
    "### Issues:\n",
    "- stemming created gibberish\n",
    "- GridSearchCV takes 2+ hours to run (up to 4)\n",
    "\n",
    "---\n",
    "\n",
    "### Try:\n",
    "- turning valence into classification problem instead of regression\n",
    "    - round lable values to tens place and you'll have 10 classification categories \n",
    "- look at other regression metrics\n",
    "    - however r2 may not be good metric for this bc low correlation between variables\n",
    "    - functions ending with _score return a value to maximize, the higher the better.\n",
    "    - functions ending with _error or _loss return a value to minimize, the lower the better.\n",
    "- try lemming instead of stemming\n",
    "- try Word2Vec instead of tfidf\n",
    "- look into this for improving grid search https://scikit-learn.org/stable/modules/grid_search.html#grid-search \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksandrageorgievska/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksandrageorgievska/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aleksandrageorgievska/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import pandas for data handling\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK is our Natural-Language-Took-Kit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Libraries for helping us with strings\n",
    "import string\n",
    "# Regular Expression Library\n",
    "import re\n",
    "\n",
    "# Import text vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Import Regressor Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Import some ML helper function\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Import our metrics to evaluate our model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, max_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/labeled_lyrics_w_genres.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                  18066\n",
       "Unnamed: 0.1                                                65729\n",
       "artist                                               Beanie Sigel\n",
       "seq             As far back as I can remember I always wanted ...\n",
       "song                                           Watch Your B******\n",
       "label                                                        0.72\n",
       "genre                                                    No_genre\n",
       "Name: 18066, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[18066]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "(145250, 7)\n",
      "Pop          57357\n",
      "No_genre     42789\n",
      "Rock         26756\n",
      "Country       7440\n",
      "Rap           5959\n",
      "R&B           4773\n",
      "Non-Music      176\n",
      "Name: genre, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>artist</th>\n",
       "      <th>seq</th>\n",
       "      <th>song</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>No, no\\r\\nI ain't ever trapped out the bando\\r...</td>\n",
       "      <td>Everyday</td>\n",
       "      <td>0.626</td>\n",
       "      <td>R&amp;B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>The drinks go down and smoke goes up, I feel m...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "      <td>0.630</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>She don't live on planet Earth no more\\r\\nShe ...</td>\n",
       "      <td>The Otherside</td>\n",
       "      <td>0.240</td>\n",
       "      <td>R&amp;B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>Trippin' off that Grigio, mobbin', lights low\\...</td>\n",
       "      <td>Pinot</td>\n",
       "      <td>0.536</td>\n",
       "      <td>R&amp;B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I see a midnight panther, so gallant and so br...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "      <td>0.371</td>\n",
       "      <td>R&amp;B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1        artist  \\\n",
       "0           0             0  Elijah Blake   \n",
       "1           1             1  Elijah Blake   \n",
       "2           2             2  Elijah Blake   \n",
       "3           3             3  Elijah Blake   \n",
       "4           4             4  Elijah Blake   \n",
       "\n",
       "                                                 seq                song  \\\n",
       "0  No, no\\r\\nI ain't ever trapped out the bando\\r...            Everyday   \n",
       "1  The drinks go down and smoke goes up, I feel m...    Live Till We Die   \n",
       "2  She don't live on planet Earth no more\\r\\nShe ...       The Otherside   \n",
       "3  Trippin' off that Grigio, mobbin', lights low\\...               Pinot   \n",
       "4  I see a midnight panther, so gallant and so br...  Shadows & Diamonds   \n",
       "\n",
       "   label genre  \n",
       "0  0.626   R&B  \n",
       "1  0.630   Pop  \n",
       "2  0.240   R&B  \n",
       "3  0.536   R&B  \n",
       "4  0.371   R&B  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.isnull().sum().sum())\n",
    "print(df.duplicated().sum())\n",
    "print(df.shape)\n",
    "print(df.genre.value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing No_genre and Non-Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df[(df['genre'] == 'No_genre') | (df['genre'] == 'Non-Music')].index\n",
    "df.drop(df_dropped, inplace=True, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102285, 7)\n",
      "Pop        57357\n",
      "Rock       26756\n",
      "Country     7440\n",
      "Rap         5959\n",
      "R&B         4773\n",
      "Name: genre, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>artist</th>\n",
       "      <th>seq</th>\n",
       "      <th>song</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>No, no\\r\\nI ain't ever trapped out the bando\\r...</td>\n",
       "      <td>Everyday</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>R&amp;B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>The drinks go down and smoke goes up, I feel m...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>She don't live on planet Earth no more\\r\\nShe ...</td>\n",
       "      <td>The Otherside</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>R&amp;B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>Trippin' off that Grigio, mobbin', lights low\\...</td>\n",
       "      <td>Pinot</td>\n",
       "      <td>0.5360</td>\n",
       "      <td>R&amp;B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I see a midnight panther, so gallant and so br...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "      <td>0.3710</td>\n",
       "      <td>R&amp;B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I just want to ready your mind\\r\\n'Cause I'll ...</td>\n",
       "      <td>Uno</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>R&amp;B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Elis</td>\n",
       "      <td>Dieses ist lange her.\\r\\nDa ich deine schmalen...</td>\n",
       "      <td>Abendlied</td>\n",
       "      <td>0.3330</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Elis</td>\n",
       "      <td>A child is born\\r\\nOut of the womb of a mother...</td>\n",
       "      <td>Child</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Elis</td>\n",
       "      <td>Out of the darkness you came \\r\\nYou looked so...</td>\n",
       "      <td>Come to Me</td>\n",
       "      <td>0.1790</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Elis</td>\n",
       "      <td>Each night I lie in my bed \\r\\nAnd I think abo...</td>\n",
       "      <td>Do You Believe</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>Elis</td>\n",
       "      <td>Nebel zieh'n gespentisch vor \\r\\nDer Sucher se...</td>\n",
       "      <td>Engel der Nacht</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Elis</td>\n",
       "      <td>I'm a lonely stranger \\r\\nIn this world of pai...</td>\n",
       "      <td>My Only Love</td>\n",
       "      <td>0.2010</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>Elis</td>\n",
       "      <td>Schwere Tranen \\r\\nVergebens geweint \\r\\nRinne...</td>\n",
       "      <td>Sie Erfasst Mein Herz</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>Elis</td>\n",
       "      <td>Come calm my anger\\n\\nOur love is like a perfe...</td>\n",
       "      <td>Anger</td>\n",
       "      <td>0.3060</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Elis</td>\n",
       "      <td>I was walking through the night\\nSuddenly I re...</td>\n",
       "      <td>Black Angel</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Unnamed: 0.1        artist  \\\n",
       "0            0             0  Elijah Blake   \n",
       "1            1             1  Elijah Blake   \n",
       "2            2             2  Elijah Blake   \n",
       "3            3             3  Elijah Blake   \n",
       "4            4             4  Elijah Blake   \n",
       "5            5             5  Elijah Blake   \n",
       "7            7             7          Elis   \n",
       "8            8             8          Elis   \n",
       "9            9             9          Elis   \n",
       "10          10            10          Elis   \n",
       "11          11            11          Elis   \n",
       "12          12            12          Elis   \n",
       "13          13            13          Elis   \n",
       "14          14            14          Elis   \n",
       "15          15            15          Elis   \n",
       "\n",
       "                                                  seq                   song  \\\n",
       "0   No, no\\r\\nI ain't ever trapped out the bando\\r...               Everyday   \n",
       "1   The drinks go down and smoke goes up, I feel m...       Live Till We Die   \n",
       "2   She don't live on planet Earth no more\\r\\nShe ...          The Otherside   \n",
       "3   Trippin' off that Grigio, mobbin', lights low\\...                  Pinot   \n",
       "4   I see a midnight panther, so gallant and so br...     Shadows & Diamonds   \n",
       "5   I just want to ready your mind\\r\\n'Cause I'll ...                    Uno   \n",
       "7   Dieses ist lange her.\\r\\nDa ich deine schmalen...              Abendlied   \n",
       "8   A child is born\\r\\nOut of the womb of a mother...                  Child   \n",
       "9   Out of the darkness you came \\r\\nYou looked so...             Come to Me   \n",
       "10  Each night I lie in my bed \\r\\nAnd I think abo...         Do You Believe   \n",
       "11  Nebel zieh'n gespentisch vor \\r\\nDer Sucher se...        Engel der Nacht   \n",
       "12  I'm a lonely stranger \\r\\nIn this world of pai...           My Only Love   \n",
       "13  Schwere Tranen \\r\\nVergebens geweint \\r\\nRinne...  Sie Erfasst Mein Herz   \n",
       "14  Come calm my anger\\n\\nOur love is like a perfe...                  Anger   \n",
       "15  I was walking through the night\\nSuddenly I re...            Black Angel   \n",
       "\n",
       "     label genre  \n",
       "0   0.6260   R&B  \n",
       "1   0.6300   Pop  \n",
       "2   0.2400   R&B  \n",
       "3   0.5360   R&B  \n",
       "4   0.3710   R&B  \n",
       "5   0.3210   R&B  \n",
       "7   0.3330   Pop  \n",
       "8   0.5060   Pop  \n",
       "9   0.1790   Pop  \n",
       "10  0.2090   Pop  \n",
       "11  0.3210   Pop  \n",
       "12  0.2010   Pop  \n",
       "13  0.2180   Pop  \n",
       "14  0.3060   Pop  \n",
       "15  0.0958   Pop  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.genre.value_counts())\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Cleaning (Text Pre Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. function that makes all text lowercase.\n",
    "def make_lowercase(test_string):\n",
    "    return test_string.lower()\n",
    "\n",
    "# 2. function that removes all punctuation. \n",
    "def remove_punc(test_string):\n",
    "    test_string = re.sub(r'[^\\w\\s]', '', test_string)\n",
    "    return test_string\n",
    "\n",
    "# 3. function that removes all stopwords.\n",
    "def remove_stopwords(test_string):\n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(test_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "    \n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        \n",
    "        # Check if word is not in stopwords. Stopwords was imported from nltk.corpus\n",
    "        if word not in stopwords:\n",
    "            \n",
    "            # If word not in stopwords, append to our valid_words\n",
    "            valid_words.append(word)\n",
    "\n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string\n",
    "\n",
    "# 4. function to break words into their stem words\n",
    "def stem_words(a_string):\n",
    "    # Initalize our Stemmer\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(a_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "\n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        # Stem the word\n",
    "        stemmed_word = porter.stem(word) #from nltk.stem import PorterStemmer\n",
    "        \n",
    "        # Append stemmed word to our valid_words\n",
    "        valid_words.append(stemmed_word)\n",
    "        \n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline function \n",
    "\n",
    "def text_processing_pipeline(a_string):\n",
    "    a_string = make_lowercase(a_string)\n",
    "    a_string = remove_punc(a_string)\n",
    "    #a_string = stem_words(a_string) #removing stem_words for now because making lyrics gibberish\n",
    "    a_string = remove_stopwords(a_string)\n",
    "    return a_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing pipeline \n",
    "\n",
    "df['seq_clean'] = df['seq'].apply(text_processing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102285, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>artist</th>\n",
       "      <th>seq</th>\n",
       "      <th>song</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "      <th>seq_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>No, no\\r\\nI ain't ever trapped out the bando\\r...</td>\n",
       "      <td>Everyday</td>\n",
       "      <td>0.626</td>\n",
       "      <td>R&amp;B</td>\n",
       "      <td>aint ever trapped bando oh lord dont get wrong...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1        artist  \\\n",
       "0           0             0  Elijah Blake   \n",
       "\n",
       "                                                 seq      song  label genre  \\\n",
       "0  No, no\\r\\nI ain't ever trapped out the bando\\r...  Everyday  0.626   R&B   \n",
       "\n",
       "                                           seq_clean  \n",
       "0  aint ever trapped bando oh lord dont get wrong...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [aint, ever, trapped, bando, oh, lord, dont, g...\n",
       "1    [drinks, go, smoke, goes, feel, got, let, go, ...\n",
       "2    [dont, live, planet, earth, found, love, venus...\n",
       "3    [trippin, grigio, mobbin, lights, low, trippin...\n",
       "4    [see, midnight, panther, gallant, brave, found...\n",
       "Name: seq_clean, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.seq_clean.apply(lambda x: word_tokenize(x))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102285,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(X, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1083510289, 1224887500)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(X,total_examples= 15000, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unhappy', 0.6109914183616638),\n",
       " ('glad', 0.5998392701148987),\n",
       " ('sequestered', 0.5725224018096924),\n",
       " ('merriest', 0.5571557879447937),\n",
       " ('smiling', 0.5531657934188843),\n",
       " ('birthday', 0.5460292100906372),\n",
       " ('loveylove', 0.5429710149765015),\n",
       " ('gay', 0.5317879915237427),\n",
       " ('matzofarian', 0.5311346054077148),\n",
       " ('sad', 0.5262659192085266)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive ='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word2vec to create a lyric vector by taking the average of words present in lyric\n",
    "\n",
    "Reference to method used https://www.kaggle.com/code/nitin194/twitter-sentiment-analysis-word2vec-doc2vec/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2VecKeyedVectors' object has no attribute 'key_to_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-d08b44c43be5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlyric_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecKeyedVectors' object has no attribute 'key_to_index'"
     ]
    }
   ],
   "source": [
    "w2v_words = list(model.wv.key_to_index)\n",
    "def lyric_vector(tokens, size):\n",
    "    sent = np.zeros(200)\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in w2v_words:\n",
    "            vec = model.wv[word]\n",
    "            sent += vec\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        sent /= count\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['seq_clean'].values\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling smaller batches from dataframe for faster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #function to randomly sample n values from each genre for smaller random forest testing\n",
    "\n",
    "# def genre_sample(dataframe, k):\n",
    "#     #make an empty dataframe\n",
    "#     df_genre_sample = pd.DataFrame(columns = ['Unnamed: 0', 'artist', 'seq', 'song', 'label', 'genre', 'seq_clean'])\n",
    "    \n",
    "#     genres = ['R&B', 'Pop', 'Rap', 'Rock', 'Country']\n",
    "#     for genre in genres:\n",
    "#          df_genre_sample = df_genre_sample.append((dataframe[dataframe[\"genre\"]==genre].sample(n=k)))\n",
    "    \n",
    "#     return df_genre_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sampling from the dataframe, k is the # of samples from each genre\n",
    "\n",
    "# df_sampled = genre_sample(df, k=500)\n",
    "# print(df_sampled.shape)\n",
    "\n",
    "# #checking correct amounts of samples per genre were obtained\n",
    "# print(df_sampled.genre.value_counts())\n",
    "# df_sampled.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Regression Models for label prediction:\n",
    "- label = float scale (0-1) which signifies valence \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor\n",
    "Gradient boosting is a technique for repeatedly adding decision trees so that the next decision tree corrects the previous decision tree error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to sample the data based on k size, train_test_split, fit model, print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to sample the data based on k size, train_test_split, fit model, print results\n",
    "\n",
    "def sample_train_test_pipeline(dataframe, model, sample_size, vectorizer):\n",
    "    \n",
    "    #define an empty dataframe\n",
    "    df_sampled = pd.DataFrame(columns = ['Unnamed: 0', 'artist', 'seq', 'song', 'label', 'genre', 'seq_clean'])\n",
    "    \n",
    "    # 1) sample the data based on k size\n",
    "    all_genres = ['R&B', 'Pop', 'Rap', 'Rock', 'Country']\n",
    "    for genres in all_genres:\n",
    "         df_sampled=df_sampled.append((dataframe[dataframe[\"genre\"]==genres].sample(n=sample_size)))\n",
    "\n",
    "    # 2) print the shape, head, and value counts\n",
    "    print('\\nThe shape of your dataframe is: ', df_sampled.shape, '\\n')\n",
    "    print('\\nThe dataframe looks like: \\n', df_sampled.head(2), '\\n')\n",
    "    print('Checking each genre has the correct sample size:\\n\\n', df_sampled[\"genre\"].value_counts(), '\\n')\n",
    "    \n",
    "    # 3) assign the X and y labels\n",
    "    X_sampled = df_sampled['seq_clean'].values\n",
    "    y_sampled = df_sampled['label'].values\n",
    "    \n",
    "    # 4) train_test_split\n",
    "    X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(X_sampled, y_sampled, \n",
    "                                                                                    test_size=0.33, random_state=42)\n",
    "    # 5) vectorize\n",
    "    vectorizer = vectorizer\n",
    "    vectorizer.fit(X_train_sample)\n",
    "\n",
    "    X_train_sample = vectorizer.transform(X_train_sample)\n",
    "    X_test_sample = vectorizer.transform(X_test_sample)\n",
    "\n",
    "    print('\\nThe shape and type of X_train_sample: ', X_train_sample.shape, type(X_train_sample))\n",
    "    \n",
    "    # 6) define model\n",
    "    model = model\n",
    "    \n",
    "    # 7) fit model\n",
    "    print('\\nFitting the model...')\n",
    "    model.fit(X_train_sample, y_train_sample)\n",
    "    \n",
    "    y_pred_sample = model.predict(X_test_sample)\n",
    "    \n",
    "    # 8) print results \n",
    "    print('\\nPrinting metrics...')\n",
    "    model_mse = mean_squared_error(y_test_sample, y_pred_sample)\n",
    "    model_me = max_error(y_test_sample, y_pred_sample)\n",
    "    \n",
    "    print('\\nRESULTS...')\n",
    "    print('For a sample size of', sample_size, ': ')\n",
    "    print('The Mean Squared Error of the ', model, 'model was: {0}'.format(model_mse))\n",
    "    print('The Max Error of the ', model, 'model was: {0}'.format(model_me))\n",
    "    \n",
    "    print('\\nDone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of your dataframe is:  (500, 8) \n",
      "\n",
      "\n",
      "The dataframe looks like: \n",
      "        Unnamed: 0           artist  \\\n",
      "143093       1993  Jesse McCartney   \n",
      "127867       3367       Diana Ross   \n",
      "\n",
      "                                                      seq              song  \\\n",
      "143093  Yeah yeah oh yeah yeah\\n\\nHow do you feel when...         Checkmate   \n",
      "127867  Dream, love is only in a dream, remember\\r\\nRe...  Remember Reprise   \n",
      "\n",
      "         label genre                                          seq_clean  \\\n",
      "143093  0.1460   R&B  yeah yeah oh yeah yeah feel king got ta lie fo...   \n",
      "127867  0.0757   R&B  dream love dream remember remember life never ...   \n",
      "\n",
      "        Unnamed: 0.1  \n",
      "143093      106560.0  \n",
      "127867       72571.0   \n",
      "\n",
      "Checking each genre has the correct sample size:\n",
      "\n",
      " Rock       100\n",
      "R&B        100\n",
      "Rap        100\n",
      "Country    100\n",
      "Pop        100\n",
      "Name: genre, dtype: int64 \n",
      "\n",
      "\n",
      "The shape and type of X_train_sample:  (335, 7568) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "Fitting the model...\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0605            1.36s\n",
      "         2           0.0586            1.29s\n",
      "         3           0.0569            1.29s\n",
      "         4           0.0552            1.26s\n",
      "         5           0.0540            1.22s\n",
      "         6           0.0523            1.21s\n",
      "         7           0.0511            1.20s\n",
      "         8           0.0495            1.20s\n",
      "         9           0.0485            1.19s\n",
      "        10           0.0475            1.18s\n",
      "        11           0.0468            1.16s\n",
      "        12           0.0460            1.15s\n",
      "        13           0.0450            1.14s\n",
      "        14           0.0443            1.14s\n",
      "        15           0.0434            1.13s\n",
      "        16           0.0425            1.13s\n",
      "        17           0.0414            1.12s\n",
      "        18           0.0408            1.11s\n",
      "        19           0.0401            1.10s\n",
      "        20           0.0394            1.10s\n",
      "        21           0.0387            1.09s\n",
      "        22           0.0381            1.09s\n",
      "        23           0.0375            1.08s\n",
      "        24           0.0370            1.08s\n",
      "        25           0.0364            1.07s\n",
      "        26           0.0359            1.06s\n",
      "        27           0.0353            1.06s\n",
      "        28           0.0348            1.05s\n",
      "        29           0.0343            1.04s\n",
      "        30           0.0339            1.03s\n",
      "        31           0.0335            1.03s\n",
      "        32           0.0329            1.02s\n",
      "        33           0.0324            1.02s\n",
      "        34           0.0319            1.01s\n",
      "        35           0.0315            1.00s\n",
      "        36           0.0311            1.00s\n",
      "        37           0.0307            0.99s\n",
      "        38           0.0303            0.98s\n",
      "        39           0.0298            0.98s\n",
      "        40           0.0294            0.97s\n",
      "        41           0.0289            0.96s\n",
      "        42           0.0285            0.96s\n",
      "        43           0.0282            0.95s\n",
      "        44           0.0278            0.95s\n",
      "        45           0.0274            0.94s\n",
      "        46           0.0269            0.94s\n",
      "        47           0.0266            0.93s\n",
      "        48           0.0263            0.92s\n",
      "        49           0.0259            0.91s\n",
      "        50           0.0256            0.91s\n",
      "        51           0.0253            0.90s\n",
      "        52           0.0250            0.90s\n",
      "        53           0.0247            0.89s\n",
      "        54           0.0244            0.88s\n",
      "        55           0.0241            0.88s\n",
      "        56           0.0238            0.87s\n",
      "        57           0.0235            0.86s\n",
      "        58           0.0232            0.86s\n",
      "        59           0.0229            0.85s\n",
      "        60           0.0227            0.84s\n",
      "        61           0.0224            0.84s\n",
      "        62           0.0220            0.83s\n",
      "        63           0.0217            0.83s\n",
      "        64           0.0215            0.82s\n",
      "        65           0.0212            0.81s\n",
      "        66           0.0210            0.81s\n",
      "        67           0.0208            0.80s\n",
      "        68           0.0206            0.79s\n",
      "        69           0.0203            0.79s\n",
      "        70           0.0201            0.78s\n",
      "        71           0.0196            0.78s\n",
      "        72           0.0194            0.77s\n",
      "        73           0.0191            0.76s\n",
      "        74           0.0188            0.76s\n",
      "        75           0.0186            0.75s\n",
      "        76           0.0184            0.74s\n",
      "        77           0.0182            0.74s\n",
      "        78           0.0180            0.73s\n",
      "        79           0.0178            0.73s\n",
      "        80           0.0176            0.72s\n",
      "        81           0.0174            0.71s\n",
      "        82           0.0172            0.71s\n",
      "        83           0.0170            0.70s\n",
      "        84           0.0168            0.69s\n",
      "        85           0.0166            0.69s\n",
      "        86           0.0165            0.68s\n",
      "        87           0.0163            0.68s\n",
      "        88           0.0161            0.67s\n",
      "        89           0.0159            0.66s\n",
      "        90           0.0158            0.66s\n",
      "        91           0.0156            0.65s\n",
      "        92           0.0154            0.64s\n",
      "        93           0.0153            0.64s\n",
      "        94           0.0151            0.63s\n",
      "        95           0.0149            0.63s\n",
      "        96           0.0147            0.62s\n",
      "        97           0.0145            0.62s\n",
      "        98           0.0144            0.61s\n",
      "        99           0.0143            0.60s\n",
      "       100           0.0141            0.60s\n",
      "       101           0.0140            0.59s\n",
      "       102           0.0138            0.59s\n",
      "       103           0.0136            0.58s\n",
      "       104           0.0135            0.57s\n",
      "       105           0.0134            0.57s\n",
      "       106           0.0132            0.56s\n",
      "       107           0.0131            0.56s\n",
      "       108           0.0130            0.55s\n",
      "       109           0.0128            0.54s\n",
      "       110           0.0127            0.54s\n",
      "       111           0.0125            0.53s\n",
      "       112           0.0123            0.53s\n",
      "       113           0.0122            0.52s\n",
      "       114           0.0121            0.51s\n",
      "       115           0.0119            0.51s\n",
      "       116           0.0118            0.50s\n",
      "       117           0.0117            0.50s\n",
      "       118           0.0116            0.49s\n",
      "       119           0.0114            0.48s\n",
      "       120           0.0113            0.48s\n",
      "       121           0.0112            0.47s\n",
      "       122           0.0111            0.47s\n",
      "       123           0.0109            0.46s\n",
      "       124           0.0108            0.45s\n",
      "       125           0.0107            0.45s\n",
      "       126           0.0106            0.44s\n",
      "       127           0.0104            0.44s\n",
      "       128           0.0103            0.43s\n",
      "       129           0.0102            0.42s\n",
      "       130           0.0101            0.42s\n",
      "       131           0.0100            0.41s\n",
      "       132           0.0098            0.41s\n",
      "       133           0.0097            0.40s\n",
      "       134           0.0096            0.39s\n",
      "       135           0.0095            0.39s\n",
      "       136           0.0094            0.38s\n",
      "       137           0.0093            0.38s\n",
      "       138           0.0092            0.37s\n",
      "       139           0.0091            0.36s\n",
      "       140           0.0090            0.36s\n",
      "       141           0.0089            0.35s\n",
      "       142           0.0088            0.34s\n",
      "       143           0.0087            0.34s\n",
      "       144           0.0086            0.33s\n",
      "       145           0.0085            0.33s\n",
      "       146           0.0084            0.32s\n",
      "       147           0.0083            0.31s\n",
      "       148           0.0082            0.31s\n",
      "       149           0.0081            0.30s\n",
      "       150           0.0080            0.30s\n",
      "       151           0.0079            0.29s\n",
      "       152           0.0078            0.29s\n",
      "       153           0.0078            0.28s\n",
      "       154           0.0077            0.27s\n",
      "       155           0.0076            0.27s\n",
      "       156           0.0075            0.26s\n",
      "       157           0.0075            0.26s\n",
      "       158           0.0074            0.25s\n",
      "       159           0.0073            0.24s\n",
      "       160           0.0073            0.24s\n",
      "       161           0.0072            0.23s\n",
      "       162           0.0071            0.23s\n",
      "       163           0.0071            0.22s\n",
      "       164           0.0070            0.21s\n",
      "       165           0.0069            0.21s\n",
      "       166           0.0069            0.20s\n",
      "       167           0.0068            0.20s\n",
      "       168           0.0067            0.19s\n",
      "       169           0.0066            0.18s\n",
      "       170           0.0066            0.18s\n",
      "       171           0.0065            0.17s\n",
      "       172           0.0065            0.17s\n",
      "       173           0.0064            0.16s\n",
      "       174           0.0063            0.15s\n",
      "       175           0.0063            0.15s\n",
      "       176           0.0062            0.14s\n",
      "       177           0.0061            0.14s\n",
      "       178           0.0061            0.13s\n",
      "       179           0.0060            0.12s\n",
      "       180           0.0060            0.12s\n",
      "       181           0.0059            0.11s\n",
      "       182           0.0059            0.11s\n",
      "       183           0.0058            0.10s\n",
      "       184           0.0058            0.09s\n",
      "       185           0.0057            0.09s\n",
      "       186           0.0056            0.08s\n",
      "       187           0.0056            0.08s\n",
      "       188           0.0055            0.07s\n",
      "       189           0.0055            0.07s\n",
      "       190           0.0054            0.06s\n",
      "       191           0.0054            0.05s\n",
      "       192           0.0053            0.05s\n",
      "       193           0.0053            0.04s\n",
      "       194           0.0052            0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       195           0.0052            0.03s\n",
      "       196           0.0051            0.02s\n",
      "       197           0.0051            0.02s\n",
      "       198           0.0050            0.01s\n",
      "       199           0.0050            0.01s\n",
      "       200           0.0049            0.00s\n",
      "\n",
      "Printing metrics...\n",
      "\n",
      "RESULTS...\n",
      "For a sample size of 100 : \n",
      "The Mean Squared Error of the  GradientBoostingRegressor(n_estimators=200, verbose=2) model was: 0.06339029791901489\n",
      "The Max Error of the  GradientBoostingRegressor(n_estimators=200, verbose=2) model was: 0.6465404962964746\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sample_train_test_pipeline(dataframe=df, model=GradientBoostingRegressor(n_estimators=200, verbose=2), \n",
    "                           sample_size=100, vectorizer=TfidfVectorizer())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of your dataframe is:  (500, 8) \n",
      "\n",
      "\n",
      "The dataframe looks like: \n",
      "        Unnamed: 0                artist  \\\n",
      "111432       3532        Bobby Caldwell   \n",
      "4370         4370  The Brothers Johnson   \n",
      "\n",
      "                                                      seq            song  \\\n",
      "111432  How long\\r\\nHow long have you been away\\r\\nOh,...        My Flame   \n",
      "4370    So glad we've got a good thing\\r\\nYou know you...  The Real Thing   \n",
      "\n",
      "        label genre                                          seq_clean  \\\n",
      "111432  0.411   R&B  long long away oh long cant find words say ive...   \n",
      "4370    0.963   R&B  glad weve got good thing know make heart sing ...   \n",
      "\n",
      "        Unnamed: 0.1  \n",
      "111432       83943.0  \n",
      "4370          4370.0   \n",
      "\n",
      "Checking each genre has the correct sample size:\n",
      "\n",
      " Rock       100\n",
      "R&B        100\n",
      "Rap        100\n",
      "Country    100\n",
      "Pop        100\n",
      "Name: genre, dtype: int64 \n",
      "\n",
      "\n",
      "The shape and type of X_train_sample:  (335, 7381) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "Fitting the model...\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0598            2.07s\n",
      "         2           0.0574            1.99s\n",
      "         3           0.0552            1.95s\n",
      "         4           0.0532            1.93s\n",
      "         5           0.0514            1.92s\n",
      "         6           0.0500            1.88s\n",
      "         7           0.0484            1.88s\n",
      "         8           0.0473            1.84s\n",
      "         9           0.0464            1.82s\n",
      "        10           0.0452            1.80s\n",
      "        11           0.0443            1.79s\n",
      "        12           0.0435            1.78s\n",
      "        13           0.0426            1.77s\n",
      "        14           0.0419            1.76s\n",
      "        15           0.0412            1.74s\n",
      "        16           0.0405            1.73s\n",
      "        17           0.0398            1.73s\n",
      "        18           0.0390            1.73s\n",
      "        19           0.0384            1.73s\n",
      "        20           0.0378            1.73s\n",
      "        21           0.0372            1.72s\n",
      "        22           0.0365            1.71s\n",
      "        23           0.0359            1.71s\n",
      "        24           0.0353            1.70s\n",
      "        25           0.0347            1.70s\n",
      "        26           0.0343            1.69s\n",
      "        27           0.0338            1.68s\n",
      "        28           0.0334            1.67s\n",
      "        29           0.0330            1.66s\n",
      "        30           0.0325            1.65s\n",
      "        31           0.0319            1.64s\n",
      "        32           0.0313            1.63s\n",
      "        33           0.0308            1.62s\n",
      "        34           0.0304            1.61s\n",
      "        35           0.0300            1.61s\n",
      "        36           0.0296            1.60s\n",
      "        37           0.0292            1.59s\n",
      "        38           0.0287            1.58s\n",
      "        39           0.0283            1.57s\n",
      "        40           0.0279            1.56s\n",
      "        41           0.0275            1.56s\n",
      "        42           0.0272            1.55s\n",
      "        43           0.0268            1.54s\n",
      "        44           0.0265            1.54s\n",
      "        45           0.0262            1.53s\n",
      "        46           0.0258            1.52s\n",
      "        47           0.0255            1.52s\n",
      "        48           0.0252            1.51s\n",
      "        49           0.0248            1.50s\n",
      "        50           0.0245            1.49s\n",
      "        51           0.0241            1.49s\n",
      "        52           0.0238            1.48s\n",
      "        53           0.0235            1.47s\n",
      "        54           0.0231            1.46s\n",
      "        55           0.0228            1.46s\n",
      "        56           0.0226            1.45s\n",
      "        57           0.0223            1.44s\n",
      "        58           0.0220            1.44s\n",
      "        59           0.0217            1.43s\n",
      "        60           0.0215            1.42s\n",
      "        61           0.0212            1.42s\n",
      "        62           0.0210            1.41s\n",
      "        63           0.0208            1.41s\n",
      "        64           0.0204            1.40s\n",
      "        65           0.0202            1.40s\n",
      "        66           0.0200            1.40s\n",
      "        67           0.0197            1.40s\n",
      "        68           0.0195            1.40s\n",
      "        69           0.0193            1.40s\n",
      "        70           0.0190            1.39s\n",
      "        71           0.0187            1.39s\n",
      "        72           0.0185            1.38s\n",
      "        73           0.0183            1.38s\n",
      "        74           0.0181            1.37s\n",
      "        75           0.0179            1.36s\n",
      "        76           0.0177            1.36s\n",
      "        77           0.0175            1.35s\n",
      "        78           0.0173            1.34s\n",
      "        79           0.0170            1.34s\n",
      "        80           0.0168            1.33s\n",
      "        81           0.0167            1.33s\n",
      "        82           0.0165            1.32s\n",
      "        83           0.0163            1.31s\n",
      "        84           0.0161            1.31s\n",
      "        85           0.0159            1.30s\n",
      "        86           0.0157            1.29s\n",
      "        87           0.0156            1.29s\n",
      "        88           0.0154            1.28s\n",
      "        89           0.0152            1.28s\n",
      "        90           0.0150            1.27s\n",
      "        91           0.0149            1.27s\n",
      "        92           0.0147            1.26s\n",
      "        93           0.0146            1.25s\n",
      "        94           0.0144            1.25s\n",
      "        95           0.0143            1.24s\n",
      "        96           0.0141            1.24s\n",
      "        97           0.0140            1.23s\n",
      "        98           0.0138            1.23s\n",
      "        99           0.0137            1.22s\n",
      "       100           0.0135            1.21s\n",
      "       101           0.0134            1.21s\n",
      "       102           0.0132            1.20s\n",
      "       103           0.0130            1.20s\n",
      "       104           0.0129            1.19s\n",
      "       105           0.0128            1.18s\n",
      "       106           0.0126            1.18s\n",
      "       107           0.0125            1.17s\n",
      "       108           0.0123            1.16s\n",
      "       109           0.0122            1.16s\n",
      "       110           0.0121            1.15s\n",
      "       111           0.0120            1.14s\n",
      "       112           0.0119            1.14s\n",
      "       113           0.0117            1.13s\n",
      "       114           0.0116            1.13s\n",
      "       115           0.0115            1.12s\n",
      "       116           0.0114            1.11s\n",
      "       117           0.0112            1.11s\n",
      "       118           0.0111            1.10s\n",
      "       119           0.0110            1.09s\n",
      "       120           0.0109            1.09s\n",
      "       121           0.0108            1.08s\n",
      "       122           0.0106            1.07s\n",
      "       123           0.0105            1.07s\n",
      "       124           0.0104            1.06s\n",
      "       125           0.0103            1.05s\n",
      "       126           0.0102            1.05s\n",
      "       127           0.0101            1.04s\n",
      "       128           0.0100            1.03s\n",
      "       129           0.0099            1.03s\n",
      "       130           0.0098            1.02s\n",
      "       131           0.0097            1.02s\n",
      "       132           0.0096            1.01s\n",
      "       133           0.0095            1.00s\n",
      "       134           0.0094            1.00s\n",
      "       135           0.0093            0.99s\n",
      "       136           0.0092            0.99s\n",
      "       137           0.0091            0.98s\n",
      "       138           0.0090            0.97s\n",
      "       139           0.0089            0.97s\n",
      "       140           0.0088            0.96s\n",
      "       141           0.0087            0.95s\n",
      "       142           0.0086            0.95s\n",
      "       143           0.0085            0.94s\n",
      "       144           0.0084            0.94s\n",
      "       145           0.0084            0.93s\n",
      "       146           0.0083            0.92s\n",
      "       147           0.0082            0.92s\n",
      "       148           0.0081            0.91s\n",
      "       149           0.0080            0.90s\n",
      "       150           0.0080            0.90s\n",
      "       151           0.0079            0.89s\n",
      "       152           0.0078            0.89s\n",
      "       153           0.0077            0.88s\n",
      "       154           0.0076            0.87s\n",
      "       155           0.0076            0.87s\n",
      "       156           0.0075            0.86s\n",
      "       157           0.0074            0.86s\n",
      "       158           0.0073            0.85s\n",
      "       159           0.0073            0.84s\n",
      "       160           0.0072            0.84s\n",
      "       161           0.0071            0.83s\n",
      "       162           0.0070            0.83s\n",
      "       163           0.0070            0.82s\n",
      "       164           0.0069            0.81s\n",
      "       165           0.0069            0.81s\n",
      "       166           0.0068            0.80s\n",
      "       167           0.0067            0.79s\n",
      "       168           0.0067            0.79s\n",
      "       169           0.0066            0.78s\n",
      "       170           0.0065            0.78s\n",
      "       171           0.0064            0.77s\n",
      "       172           0.0063            0.76s\n",
      "       173           0.0063            0.76s\n",
      "       174           0.0062            0.75s\n",
      "       175           0.0062            0.75s\n",
      "       176           0.0061            0.74s\n",
      "       177           0.0060            0.73s\n",
      "       178           0.0060            0.73s\n",
      "       179           0.0059            0.72s\n",
      "       180           0.0059            0.71s\n",
      "       181           0.0058            0.71s\n",
      "       182           0.0058            0.70s\n",
      "       183           0.0057            0.70s\n",
      "       184           0.0057            0.69s\n",
      "       185           0.0056            0.68s\n",
      "       186           0.0055            0.68s\n",
      "       187           0.0055            0.67s\n",
      "       188           0.0054            0.67s\n",
      "       189           0.0054            0.66s\n",
      "       190           0.0054            0.65s\n",
      "       191           0.0053            0.65s\n",
      "       192           0.0053            0.64s\n",
      "       193           0.0052            0.64s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       194           0.0051            0.63s\n",
      "       195           0.0051            0.62s\n",
      "       196           0.0050            0.62s\n",
      "       197           0.0050            0.61s\n",
      "       198           0.0049            0.61s\n",
      "       199           0.0049            0.60s\n",
      "       200           0.0048            0.59s\n",
      "       201           0.0048            0.59s\n",
      "       202           0.0048            0.58s\n",
      "       203           0.0047            0.58s\n",
      "       204           0.0047            0.57s\n",
      "       205           0.0046            0.56s\n",
      "       206           0.0046            0.56s\n",
      "       207           0.0045            0.55s\n",
      "       208           0.0045            0.55s\n",
      "       209           0.0044            0.54s\n",
      "       210           0.0044            0.53s\n",
      "       211           0.0043            0.53s\n",
      "       212           0.0043            0.52s\n",
      "       213           0.0043            0.52s\n",
      "       214           0.0042            0.51s\n",
      "       215           0.0042            0.50s\n",
      "       216           0.0041            0.50s\n",
      "       217           0.0041            0.49s\n",
      "       218           0.0041            0.49s\n",
      "       219           0.0040            0.48s\n",
      "       220           0.0040            0.47s\n",
      "       221           0.0040            0.47s\n",
      "       222           0.0039            0.46s\n",
      "       223           0.0039            0.46s\n",
      "       224           0.0038            0.45s\n",
      "       225           0.0038            0.44s\n",
      "       226           0.0038            0.44s\n",
      "       227           0.0037            0.43s\n",
      "       228           0.0037            0.43s\n",
      "       229           0.0037            0.42s\n",
      "       230           0.0036            0.41s\n",
      "       231           0.0036            0.41s\n",
      "       232           0.0036            0.40s\n",
      "       233           0.0035            0.40s\n",
      "       234           0.0035            0.39s\n",
      "       235           0.0035            0.38s\n",
      "       236           0.0034            0.38s\n",
      "       237           0.0034            0.37s\n",
      "       238           0.0034            0.37s\n",
      "       239           0.0033            0.36s\n",
      "       240           0.0033            0.36s\n",
      "       241           0.0033            0.35s\n",
      "       242           0.0032            0.34s\n",
      "       243           0.0032            0.34s\n",
      "       244           0.0032            0.33s\n",
      "       245           0.0031            0.33s\n",
      "       246           0.0031            0.32s\n",
      "       247           0.0031            0.31s\n",
      "       248           0.0030            0.31s\n",
      "       249           0.0030            0.30s\n",
      "       250           0.0030            0.30s\n",
      "       251           0.0030            0.29s\n",
      "       252           0.0029            0.28s\n",
      "       253           0.0029            0.28s\n",
      "       254           0.0029            0.27s\n",
      "       255           0.0029            0.27s\n",
      "       256           0.0028            0.26s\n",
      "       257           0.0028            0.25s\n",
      "       258           0.0028            0.25s\n",
      "       259           0.0028            0.24s\n",
      "       260           0.0027            0.24s\n",
      "       261           0.0027            0.23s\n",
      "       262           0.0027            0.23s\n",
      "       263           0.0027            0.22s\n",
      "       264           0.0026            0.21s\n",
      "       265           0.0026            0.21s\n",
      "       266           0.0026            0.20s\n",
      "       267           0.0026            0.20s\n",
      "       268           0.0025            0.19s\n",
      "       269           0.0025            0.18s\n",
      "       270           0.0025            0.18s\n",
      "       271           0.0025            0.17s\n",
      "       272           0.0024            0.17s\n",
      "       273           0.0024            0.16s\n",
      "       274           0.0024            0.15s\n",
      "       275           0.0024            0.15s\n",
      "       276           0.0024            0.14s\n",
      "       277           0.0023            0.14s\n",
      "       278           0.0023            0.13s\n",
      "       279           0.0023            0.12s\n",
      "       280           0.0023            0.12s\n",
      "       281           0.0022            0.11s\n",
      "       282           0.0022            0.11s\n",
      "       283           0.0022            0.10s\n",
      "       284           0.0022            0.09s\n",
      "       285           0.0021            0.09s\n",
      "       286           0.0021            0.08s\n",
      "       287           0.0021            0.08s\n",
      "       288           0.0021            0.07s\n",
      "       289           0.0021            0.07s\n",
      "       290           0.0021            0.06s\n",
      "       291           0.0020            0.05s\n",
      "       292           0.0020            0.05s\n",
      "       293           0.0020            0.04s\n",
      "       294           0.0020            0.04s\n",
      "       295           0.0020            0.03s\n",
      "       296           0.0020            0.02s\n",
      "       297           0.0019            0.02s\n",
      "       298           0.0019            0.01s\n",
      "       299           0.0019            0.01s\n",
      "       300           0.0019            0.00s\n",
      "\n",
      "Printing metrics...\n",
      "\n",
      "RESULTS...\n",
      "For a sample size of 100 : \n",
      "The Mean Squared Error of the  GradientBoostingRegressor(n_estimators=300, verbose=2) model was: 0.06416713019690369\n",
      "The Max Error of the  GradientBoostingRegressor(n_estimators=300, verbose=2) model was: 0.7093920998022567\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sample_train_test_pipeline(dataframe=df, model=GradientBoostingRegressor(n_estimators=300, verbose=2), \n",
    "                           sample_size=100, vectorizer=TfidfVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size of 4773 and no hyperparameters\n",
      "\n",
      "\n",
      "The shape of your dataframe is:  (23865, 8) \n",
      "\n",
      "\n",
      "The dataframe looks like: \n",
      "        Unnamed: 0          artist  \\\n",
      "131518       2868    Annie Lennox   \n",
      "144492       3392  Mint Condition   \n",
      "\n",
      "                                                      seq  \\\n",
      "131518  Angels from the realms of glory\\nWing your fli...   \n",
      "144492  I wait for the day A sweet gentle sway rocks y...   \n",
      "\n",
      "                                   song  label genre  \\\n",
      "131518  Angels from the Realms of Glory  0.300   R&B   \n",
      "144492               U Send Me Swingin'  0.346   R&B   \n",
      "\n",
      "                                                seq_clean  Unnamed: 0.1  \n",
      "131518  angels realms glory wing flight earth ye sang ...      155696.0  \n",
      "144492  wait day sweet gentle sway rocks love right wa...      158019.0   \n",
      "\n",
      "Checking each genre has the correct sample size:\n",
      "\n",
      " Rock       4773\n",
      "R&B        4773\n",
      "Rap        4773\n",
      "Country    4773\n",
      "Pop        4773\n",
      "Name: genre, dtype: int64 \n",
      "\n",
      "\n",
      "The shape and type of X_train_sample:  (15989, 62362) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "Fitting the model...\n",
      "\n",
      "Printing metrics...\n",
      "\n",
      "For a sample size of 4773 : \n",
      "\n",
      "The Mean Squared Error of the  GradientBoostingRegressor() model was: 0.051454211801993846\n",
      "\n",
      "The Max Error of the  GradientBoostingRegressor() model was: 0.5878491165172922\n",
      "\n",
      "Done!\n",
      "\n",
      "sample size of 4773 with hyperparameters:\n",
      "\n",
      "The shape of your dataframe is:  (23865, 8) \n",
      "\n",
      "\n",
      "The dataframe looks like: \n",
      "        Unnamed: 0        artist  \\\n",
      "23562       23562       OutKast   \n",
      "139711       6911  Otis Redding   \n",
      "\n",
      "                                                      seq  \\\n",
      "23562   Sophistophone, aristocrats\\nDistinguished star...   \n",
      "139711  I can't get no satisfaction\\r\\nI can't get no ...   \n",
      "\n",
      "                                 song  label genre  \\\n",
      "23562                   Behold a Lady  0.709   R&B   \n",
      "139711  (I Can't Get No) Satisfaction  0.867   R&B   \n",
      "\n",
      "                                                seq_clean  Unnamed: 0.1  \n",
      "23562   sophistophone aristocrats distinguished stars ...       36163.0  \n",
      "139711  cant get satisfaction cant get satisfaction tr...       35812.0   \n",
      "\n",
      "Checking each genre has the correct sample size:\n",
      "\n",
      " Rock       4773\n",
      "R&B        4773\n",
      "Rap        4773\n",
      "Country    4773\n",
      "Pop        4773\n",
      "Name: genre, dtype: int64 \n",
      "\n",
      "\n",
      "The shape and type of X_train_sample:  (15989, 62298) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "Fitting the model...\n",
      "\n",
      "Printing metrics...\n",
      "\n",
      "For a sample size of 4773 : \n",
      "\n",
      "The Mean Squared Error of the  GradientBoostingRegressor() model was: 0.051766960650948\n",
      "\n",
      "The Max Error of the  GradientBoostingRegressor() model was: 0.6073921372467539\n",
      "\n",
      "Done!\n",
      "DONE!!\n"
     ]
    }
   ],
   "source": [
    "print('sample size of 4773 and no hyperparameters\\n')\n",
    "\n",
    "sample_train_test_pipeline(dataframe=df, model=GradientBoostingRegressor(), sample_size=4773, vectorizer=TfidfVectorizer())\n",
    "\n",
    "print('\\nsample size of 4773 with hyperparameters:')\n",
    "sample_train_test_pipeline(dataframe=df, model=GradientBoostingRegressor(learning_rate=0.1, max_depth=3, n_estimators= 100), sample_size=4773, vectorizer=TfidfVectorizer())\n",
    "\n",
    "print('\\nsample size of 4773 with hyperparameters:')\n",
    "sample_train_test_pipeline(dataframe=df, model=GradientBoostingRegressor(learning_rate=0.1, n_estimators= 100), sample_size=4773, vectorizer=TfidfVectorizer())\n",
    "\n",
    "print('DONE!!')\n",
    "\n",
    "# conclusion the hyperparameters made the max error worse so NOT BEST PARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning of Gradiet Boosting Regressor with GridSearchCV\n",
    "- need to run overnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a function for hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_grid= {'n_estimators': [100, 500, 1000],\n",
    "                'learning_rate' : [0.05, 0.1, 0.15],\n",
    "                'max_depth': [3, 4, 5]\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Do not run the next cell unless you have 2 hours to kill*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Grid Search ... \")\n",
    "\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "gb_grid = GridSearchCV(n_jobs = -1, estimator = gb_regressor, param_grid= gb_param_grid, cv=3, scoring= 'neg_mean_squared_error')\n",
    "\n",
    "print(\"Running the fit..\")\n",
    "\n",
    "gb_grid_search = gb_grid.fit(X_train_sample3, y_train_sample3)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "best_score3 = gb_grid_search.best_score_\n",
    "print(\"The best score is: \", best_score3)\n",
    "\n",
    "gb_best_params = gb_grid_search.best_params_\n",
    "print(\"The best parameters are: \", gb_best_params)\n",
    "\n",
    "# The best score is:  0.07331262579138897\n",
    "# The best parameters are:  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Larger GB Test on 1687 samples from each Genre\n",
    "### with optimized hyper parameters {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from the dataframe, k is 1687 which is the max number of samples from R&B the smallest Genre pool \n",
    "\n",
    "df_sampled4 = genre_sample(df, k=1687)\n",
    "print(df_sampled4.shape)\n",
    "df_sampled4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking correct amounts of samples per genre were obtained\n",
    "\n",
    "df_sampled4.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sampled4 = df_sampled4['seq_clean'].values\n",
    "\n",
    "y_sampled4 = df_sampled4['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample4, X_test_sample4, y_train_sample4, y_test_sample4 = train_test_split(X_sampled4, y_sampled4, \n",
    "                                                                                    test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer4 = TfidfVectorizer()\n",
    "vectorizer4.fit(X_train_sample4)\n",
    "\n",
    "X_train_sample4 = vectorizer4.transform(X_train_sample4)\n",
    "X_test_sample4 = vectorizer4.transform(X_test_sample4)\n",
    "\n",
    "print(X_train_sample4.shape, type(X_train_sample4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb2 = GradientBoostingRegressor(learning_rate=0.1, max_depth=3, n_estimators= 100)\n",
    "gb2.fit(X_train_sample4, y_train_sample4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sample4 = gb2.predict(X_test_sample4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mse2 = mean_squared_error(y_test_sample4, y_pred_sample4)\n",
    "gb_r2_2 = r2_score(y_test_sample4, y_pred_sample4)\n",
    "\n",
    "print('For a sample size of 1687 and optimal hyperparameters:')\n",
    "print('[GB] Mean Squared Error: {0}'.format(gb_mse2))\n",
    "print('[GB] R2: {0}'.format(gb_r2_2))\n",
    "\n",
    "# For a sample size of 1687 and optimal hyperparameters:\n",
    "# [GB] Mean Squared Error: 0.05200244352093515\n",
    "# [GB] R2: 0.09854691815352778"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running GB on full data set with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 3. Fit your vectorizer using your X data\n",
    "# This makes your vocab matrix\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# 4. Transform your X data using your fitted vectorizer. \n",
    "# This transforms your documents into vectors.\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train.shape, type(X))\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb3 = GradientBoostingRegressor()\n",
    "gb3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gb3.predict(X_test)\n",
    "gb3_mse = mean_squared_error(y_test, y_pred)\n",
    "gb3_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('On the full data set and optimal hyperparameter tuning:')\n",
    "print('[GB] Mean Squared Error: {0}'.format(gb3_mse))\n",
    "print('[GB] R2: {0}'.format(gb3_r2))\n",
    "print(\"\\n ...\")\n",
    "\n",
    "# On the full data set and optimal hyperparameter tuning:\n",
    "# [GB] Mean Squared Error: 0.05284822264086245\n",
    "# [GB] R2: 0.12264012250017176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion on tesing Gradient Boosting Regressors\n",
    "\n",
    "Not much gained or lost from increase in sample size. \n",
    "R2 error increased by .018 which is not too significant \n",
    "\n",
    "on sample size 500/genre and NO hyperparameter tuning:\n",
    "- [GB] Mean Squared Error: 0.05264719893512396\n",
    "- [GB] R2: 0.08043425668821236\n",
    "\n",
    "on sample size 1687/genre with optimal hyperparamers: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}:\n",
    "- [GB] Mean Squared Error: 0.05200244352093515\n",
    "- [GB] R2: 0.09854691815352778\n",
    "\n",
    "on full data set with optimal hyperparamers:\n",
    "- [GB] Mean Squared Error: 0.05284822264086245\n",
    "- [GB] R2: 0.12264012250017176\n",
    "\n",
    "** GB runs MUCH faster than RFreg and produces better r2 on the full data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using the sampled dataset for faster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sampled = df_sampled['seq_clean'].values\n",
    "\n",
    "y_sampled = df_sampled['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(X_sampled, y_sampled, \n",
    "                                                                             test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train_sample)\n",
    "\n",
    "X_train_sample = vectorizer.transform(X_train_sample)\n",
    "X_test_sample = vectorizer.transform(X_test_sample)\n",
    "\n",
    "print(X_train_sample.shape, type(X_train_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(RandomForestRegressor.get_params(rf).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the best parameters for RandomForestRegressor\n",
    "\n",
    "param_grid = {'n_estimators': [100, 1000],\n",
    "              'max_depth': [2, 8, 32, 'None']\n",
    "             }\n",
    "\n",
    "# there are more parameters to test but I was getting errors and need to investigate more\n",
    "\n",
    "# param_grid = {'criterion': ['squared_error', 'absolute_error', 'poisson'],\n",
    "#               'n_estimators': [10, 50, 100, 500, 1000], \n",
    "#               'max_depth': [2, 4, 8, 16, 32, 64],\n",
    "#               'min_samples_leaf': [1, 10, 25, 50],\n",
    "#               'bootstrap': [True, False],\n",
    "#               'min_samples_split': [0, 2, 4, 8, 16, 32]\n",
    "#              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(GridSearchCV.get_params(rf_grid).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Do not run the next cell unless you have 2+ hours to kill*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running Grid Search...')\n",
    "\n",
    "# 1. Create a RandomForestRegressor model object without supplying arguments. \n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# 2. Run a Grid Search with 3-fold cross-validation and assign the output to the object 'rf_grid'.\n",
    "#    * Pass the model and the parameter grid to GridSearchCV()\n",
    "#    * Set the number of folds to 3\n",
    "#    * Specify the scoring method\n",
    "\n",
    "rf_grid = GridSearchCV(n_jobs = -1, estimator=rf, param_grid = param_grid, cv=3, scoring='neg_mean_squared_error') \n",
    "\n",
    "# 3. Fit the model (use the 'grid' variable) on the training data and assign the fitted model to the \n",
    "#    variable 'rf_grid_search'\n",
    "\n",
    "rf_grid_search = rf_grid.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding best parameters for the Random Forest Regressor\n",
    "\n",
    "best_score = rf_grid_search.best_score_\n",
    "print(\"The best score is: \", best_score)\n",
    "\n",
    "rf_best_params = rf_grid_search.best_params_\n",
    "print(\"The best params is: \", rf_best_params)\n",
    "\n",
    "# conclusion was n_estimators=1000, bootstrap = True are best hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal Hyperparameters for RandomForestRegressor based on GridSearchCV\n",
    "\n",
    "rf_model1 = RandomForestRegressor(n_estimators=1000, bootstrap = True)\n",
    "\n",
    "# 2. Fit the model to the training data below\n",
    "rf_model1.fit(X_train_sample, y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample_pred = rf_model1.predict(X_test_sample)\n",
    "\n",
    "rf_mse = mean_squared_error(y_test_sample, y_sample_pred)\n",
    "rf_r2 = r2_score(y_test_sample, y_sample_pred)\n",
    "\n",
    "print('on sample size of 500/genre with optimal hyperparameters:')\n",
    "print('[RF] Mean Squared Error: {0}'.format(rf_mse))\n",
    "print('[RF] R2: {0}'.format(rf_r2))\n",
    "\n",
    "# on sample size of 500/genre with optimal hyperparameters:\n",
    "# - [RF] Mean Squared Error: 0.05222244210798196\n",
    "# - [RF] R2: 0.10795588207769391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the predictions of the model with NEW unseen text (not part of testing set)\n",
    "\n",
    "def rgrg_string_test(lyrics):\n",
    "    new_lyrics = text_processing_pipeline(lyrics)\n",
    "    print(\"the processed lyrics are: \", new_lyrics)\n",
    "    \n",
    "    new_text_vectorized = vectorizer.transform([new_lyrics])\n",
    "    \n",
    "    value = rf_model1.predict(new_text_vectorized)\n",
    "    print(\"Random Forest Regressor model gives a value of: \", value)\n",
    "    if(value < .50):\n",
    "        print(\"which is negative\")\n",
    "    else: \n",
    "        print(\"which is positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text1 = \"Hit me baby one more time my lonliness is killing me and I must confess I still believe\"\n",
    "test_text2 = \"Oh, baby, when you talk like that You make a woman go mad So be wise and keep on Reading the signs of my body\"\n",
    "test_text3 = \"looking out on the pouring rain I used to feel so uninspired\"\n",
    "test_text4 = \"Girl put your record on tell me your favorite song just go ahead let your hair down\"\n",
    "\n",
    "rgrg_string_test(test_text1)\n",
    "print('\\n')\n",
    "rgrg_string_test(test_text2)\n",
    "print('\\n')\n",
    "rgrg_string_test(test_text3)\n",
    "print('\\n')\n",
    "rgrg_string_test(test_text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Larger RF Test on 1687 samples from each Genre\n",
    "\n",
    "- to-do: break this testing out into a function instead of repeating code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from the dataframe, k is 1687 which is the max number of samples from R&B the smallest Genre pool \n",
    "\n",
    "df_sampled2 = genre_sample(df, k=1687)\n",
    "print(df_sampled2.shape)\n",
    "df_sampled2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking correct amounts of samples per genre were obtained\n",
    "\n",
    "df_sampled2.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sampled2 = df_sampled2['seq_clean'].values\n",
    "\n",
    "y_sampled2 = df_sampled2['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample2, X_test_sample2, y_train_sample2, y_test_sample2 = train_test_split(X_sampled2, y_sampled2, \n",
    "                                                                             test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer2.fit(X_train_sample2)\n",
    "\n",
    "X_train_sample2 = vectorizer2.transform(X_train_sample2)\n",
    "X_test_sample2 = vectorizer2.transform(X_test_sample2)\n",
    "\n",
    "print(X_train_sample2.shape, type(X_train_sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal Hyperparameters for RandomForestRegressor based on GridSearchCV\n",
    "\n",
    "rf_model2 = RandomForestRegressor(n_estimators=1000, bootstrap = True)\n",
    "\n",
    "# 2. Fit the model to the training data below\n",
    "rf_model2.fit(X_train_sample2, y_train_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample_pred2 = rf_model2.predict(X_test_sample2)\n",
    "y_sample_pred2\n",
    "\n",
    "rf_mse2 = mean_squared_error(y_test_sample2, y_sample_pred2)\n",
    "rf_r2_2 = r2_score(y_test_sample2, y_sample_pred2)\n",
    "\n",
    "print('on sample size of 1687/genre  with optimal hyperparameters:')\n",
    "print('[RF] Mean Squared Error: {0}'.format(rf_mse2))\n",
    "print('[RF] R2: {0}'.format(rf_r2_2))\n",
    "\n",
    "# on sample size 1687/genre with optimal hyperprameters:\n",
    "# - [RF] Mean Squared Error: 0.05101035414841959\n",
    "# - [RF] R2: 0.1205852034338123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion on tesing Random Forest Regressors\n",
    "\n",
    "Not much gained or lost from increase in sample size. \n",
    "R2 error increased by .02 which is not too significant \n",
    "\n",
    "on sample size 500/genre with optimal hyperparameters:\n",
    "- [RF] Mean Squared Error: 0.05222244210798196\n",
    "- [RF] R2: 0.10795588207769391\n",
    "\n",
    "on sample size 1687/genre with optimal hyperprameters:\n",
    "- [RF] Mean Squared Error: 0.05101035414841959\n",
    "- [RF] R2: 0.1205852034338123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running RF Test on Full Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 3. Fit your vectorizer using your X data\n",
    "# This makes your vocab matrix\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# 4. Transform your X data using your fitted vectorizer. \n",
    "# This transforms your documents into vectors.\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train.shape, type(X))\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal Hyperparameters for RandomForestRegressor based on GridSearchCV\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=1000, bootstrap = True)\n",
    "\n",
    "# 2. Fit the model to the training data below\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "rf_mse = mean_squared_error(y_test, y_pred)\n",
    "rf_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('on full data set with optimal hyperparameters:')\n",
    "print('[RF] Mean Squared Error: {0}'.format(rf_mse))\n",
    "print('[RF] R2: {0}'.format(rf_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end of RF testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# conclusion on tesing Random Forest Regressors\n",
    "\n",
    "Not much gained or lost from increase in sample size. \n",
    "R2 error increased by .02 which is not too significant \n",
    "\n",
    "on sample size 500/genre with optimal hyperparamers:\n",
    "- [RF] Mean Squared Error: 0.05222244210798196\n",
    "- [RF] R2: 0.10795588207769391\n",
    "\n",
    "on sample size 1687/genre with optimal hyperparamers:\n",
    "- [RF] Mean Squared Error: 0.05101035414841959\n",
    "- [RF] R2: 0.1205852034338123\n",
    "\n",
    "on full data set with optimal hyperparamers:\n",
    "- [RF] Mean Squared Error: 0.05115798019890316\n",
    "- [RF] R2: 0.15070068589697727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
